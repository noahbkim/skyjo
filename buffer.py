import dataclasses
import pathlib
import pickle
import typing

import numpy as np

import play
import skyjo as sj
import skynet
import train_utils


@dataclasses.dataclass(slots=True)
class Config:
    max_size: int
    spatial_input_shape: tuple[int, ...]
    non_spatial_input_shape: tuple[int, ...]
    action_mask_shape: tuple[int, ...]
    outcome_target_shape: tuple[int, ...]
    points_target_shape: tuple[int, ...]
    policy_target_shape: tuple[int, ...]


class ReplayBuffer:
    """A replay buffer to hold the experience generated by the played games.

        Each element in the buffer is a list of training data points generated
    from a single game. Once the buffer is full, the oldest game is discarded.

        Sampling from the buffer returns a random batch of training data points
    from across all games.
    """

    def __init__(
        self,
        max_size: int,
        spatial_input_shape: tuple[int, ...],
        non_spatial_input_shape: tuple[int, ...],
        action_mask_shape: tuple[int, ...],
        outcome_target_shape: tuple[int, ...],
        points_target_shape: tuple[int, ...],
        policy_target_shape: tuple[int, ...],
    ):
        self.max_size = max_size
        self.spatial_input_buffer = np.empty(
            (max_size, *spatial_input_shape), dtype=np.float32
        )
        self.non_spatial_input_buffer = np.empty(
            (max_size, *non_spatial_input_shape), dtype=np.float32
        )
        self.action_masks = np.empty((max_size, *action_mask_shape), dtype=np.float32)
        self.policy_target_buffer = np.empty(
            (max_size, *policy_target_shape), dtype=np.float32
        )
        self.outcome_target_buffer = np.empty(
            (max_size, *outcome_target_shape), dtype=np.float32
        )
        self.points_target_buffer = np.empty(
            (max_size, *points_target_shape), dtype=np.float32
        )
        self.count = 0

    @classmethod
    def from_config(cls, config: Config) -> typing.Self:
        return cls(
            config.max_size,
            config.spatial_input_shape,
            config.non_spatial_input_shape,
            config.action_mask_shape,
            config.outcome_target_shape,
            config.points_target_shape,
            config.policy_target_shape,
        )

    @classmethod
    def load(cls, path: pathlib.Path) -> typing.Self:
        with open(path, "rb") as f:
            return pickle.load(f)

    def __len__(self):
        return min(self.count, self.max_size)

    def add(
        self,
        game_state: sj.Skyjo,
        outcome_target: np.ndarray,
        points_target: np.ndarray,
        policy_target: np.ndarray,
    ):
        buffer_index = self.count
        if self.count >= self.max_size:
            buffer_index = self.count % self.max_size
        self.spatial_input_buffer[buffer_index] = skynet.get_spatial_state_numpy(
            game_state
        )
        self.non_spatial_input_buffer[buffer_index] = (
            skynet.get_non_spatial_state_numpy(game_state)
        )
        self.action_masks[buffer_index] = sj.actions(game_state).astype(np.float32)
        self.policy_target_buffer[buffer_index] = policy_target
        self.outcome_target_buffer[buffer_index] = outcome_target
        self.points_target_buffer[buffer_index] = points_target
        self.count += 1

    def add_game_data(self, game_data: play.GameData):
        """Adds a game's worth of training data to the buffer."""

        for game_state, outcome_target, points_target, policy_target in game_data:
            self.add(game_state, outcome_target, points_target, policy_target)

    def add_game_data_with_symmetry(self, game_data: play.GameData):
        """Adds a game's worth of training data to the buffer."""
        for game_state, outcome_target, points_target, policy_target in game_data:
            for (
                symmetric_game_state,
                symmetric_policy_target,
            ) in play.get_skyjo_symmetries(game_state, policy_target):
                self.add(
                    symmetric_game_state,
                    outcome_target,
                    points_target,
                    symmetric_policy_target,
                )

    def sample_element(self) -> train_utils.TrainingDataPoint:
        assert (
            len(self.spatial_input_buffer)
            == len(self.non_spatial_input_buffer)
            == len(self.action_masks)
            == len(self.outcome_target_buffer)
            == len(self.points_target_buffer)
            == len(self.policy_target_buffer)
        ), "All buffers must be the same length"
        assert len(self) > 0, "Buffer is empty"
        # Select a random index first
        index = np.random.randint(len(self.spatial_input_buffer))
        return (
            self.spatial_input_buffer[index],
            self.non_spatial_input_buffer[index],
            self.action_masks[index],
            self.outcome_target_buffer[index],
            self.points_target_buffer[index],
            self.policy_target_buffer[index],
        )

    def sample_batch(self, batch_size: int) -> train_utils.TrainingBatch:
        assert (
            len(self.spatial_input_buffer)
            == len(self.non_spatial_input_buffer)
            == len(self.policy_target_buffer)
            == len(self.outcome_target_buffer)
            == len(self.points_target_buffer)
            == len(self.action_masks)
        ), "All buffers must be the same length"
        assert len(self) > 0, "Buffer is empty"
        assert batch_size <= len(self), (
            f"Batch size {batch_size} cannot be larger than buffer size {len(self)} when sampling without replacement."
        )
        indices = np.random.choice(
            len(self), size=batch_size
        )  # replace=False is VERY slow
        # Retrieve elements using the sampled indices
        batch = (
            self.spatial_input_buffer[indices],
            self.non_spatial_input_buffer[indices],
            self.action_masks[indices],
            self.outcome_target_buffer[indices],
            self.points_target_buffer[indices],
            self.policy_target_buffer[indices],
        )
        return batch

    def generate_training_batches(
        self, batch_size: int, batch_count: int
    ) -> typing.Generator[train_utils.TrainingBatch, None, None]:
        for _ in range(batch_count):
            yield self.sample_batch(batch_size)

    def save(self, path: pathlib.Path):
        with open(path, "wb") as f:
            pickle.dump(self, f)
