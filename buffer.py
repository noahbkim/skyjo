import dataclasses
import pathlib
import pickle
import typing

import numpy as np

import play
import skyjo as sj
import skynet
import train_utils


@dataclasses.dataclass(slots=True)
class Config:
    max_size: int


class ReplayBuffer:
    """A replay buffer to hold the experience generated by the played games.

        Each element in the buffer is a list of training data points generated
    from a single game. Once the buffer is full, the oldest game is discarded.

        Sampling from the buffer returns a random batch of training data points
    from across all games.
    """

    def __init__(self, max_size: int):
        self.max_size = max_size
        self.spatial_input_buffer = []
        self.non_spatial_input_buffer = []
        self.policy_target_buffer = []
        self.outcome_target_buffer = []
        self.points_target_buffer = []
        self.action_masks = []
        self.count = 0

    @classmethod
    def from_config(cls, config: Config) -> "ReplayBuffer":
        return cls(config.max_size)

    @classmethod
    def load(cls, path: pathlib.Path) -> "ReplayBuffer":
        with open(path, "rb") as f:
            return pickle.load(f)

    def __len__(self):
        return len(self.spatial_input_buffer)

    def add(
        self,
        game_state: sj.Skyjo,
        policy_target: np.ndarray,
        outcome_target: np.ndarray,
        points_target: np.ndarray,
    ):
        if self.count < self.max_size:
            self.spatial_input_buffer.append(skynet.get_spatial_state_numpy(game_state))
            self.non_spatial_input_buffer.append(
                skynet.get_non_spatial_state_numpy(game_state)
            )
            self.policy_target_buffer.append(policy_target)
            self.outcome_target_buffer.append(outcome_target)
            self.points_target_buffer.append(points_target)
            self.action_masks.append(sj.actions(game_state))
        else:
            self.spatial_input_buffer[self.count % self.max_size] = (
                skynet.get_spatial_state_numpy(game_state)
            )
            self.non_spatial_input_buffer[self.count % self.max_size] = (
                skynet.get_non_spatial_state_numpy(game_state)
            )
            self.policy_target_buffer[self.count % self.max_size] = policy_target
            self.outcome_target_buffer[self.count % self.max_size] = outcome_target
            self.points_target_buffer[self.count % self.max_size] = points_target
            self.action_masks[self.count % self.max_size] = sj.actions(game_state)
        self.count += 1

    def add_game_data(self, game_data: play.GameData):
        """Adds a game's worth of training data to the buffer."""
        for game_state, policy_target, outcome_target, points_target in game_data:
            self.add(game_state, policy_target, outcome_target, points_target)

    def add_game_data_with_symmetry(self, game_data: play.GameData):
        """Adds a game's worth of training data to the buffer."""
        for game_state, policy_target, outcome_target, points_target in game_data:
            for (
                symmetric_game_state,
                symmetric_policy_target,
            ) in play.get_skyjo_symmetries(game_state, policy_target):
                self.add(
                    symmetric_game_state,
                    symmetric_policy_target,
                    outcome_target,
                    points_target,
                )

    def sample_element(self) -> train_utils.TrainingDataPoint:
        assert (
            len(self.spatial_input_buffer)
            == len(self.non_spatial_input_buffer)
            == len(self.policy_target_buffer)
            == len(self.outcome_target_buffer)
            == len(self.points_target_buffer)
            == len(self.action_masks)
        ), "All buffers must be the same length"
        assert len(self.spatial_input_buffer) > 0, "Buffer is empty"
        # Select a random index first
        index = np.random.randint(len(self.spatial_input_buffer))
        return (
            self.spatial_input_buffer[index],
            self.non_spatial_input_buffer[index],
            self.outcome_target_buffer[index],
            self.points_target_buffer[index],
            self.policy_target_buffer[index],
            self.action_masks[index],
        )

    def sample_batch(self, batch_size: int) -> train_utils.TrainingBatch:
        assert (
            len(self.spatial_input_buffer)
            == len(self.non_spatial_input_buffer)
            == len(self.policy_target_buffer)
            == len(self.outcome_target_buffer)
            == len(self.points_target_buffer)
            == len(self.action_masks)
        ), "All buffers must be the same length"
        assert len(self.spatial_input_buffer) > 0, "Buffer is empty"
        assert batch_size <= len(self.spatial_input_buffer), (
            f"Batch size {batch_size} cannot be larger than buffer size {len(self.spatial_input_buffer)} when sampling without replacement."
        )
        indices = np.random.choice(
            len(self.spatial_input_buffer), size=batch_size
        )  # replace=False is VERY slow
        # Retrieve elements using the sampled indices
        batch = (
            np.array([self.spatial_input_buffer[i] for i in indices]),
            np.array([self.non_spatial_input_buffer[i] for i in indices]),
            np.array([self.outcome_target_buffer[i] for i in indices]),
            np.array([self.points_target_buffer[i] for i in indices]),
            np.array([self.policy_target_buffer[i] for i in indices]),
            np.array([self.action_masks[i] for i in indices]),
        )
        return batch

    def generate_training_batches(
        self, batch_size: int, batch_count: int
    ) -> typing.Generator[train_utils.TrainingBatch, None, None]:
        for _ in range(batch_count):
            yield self.sample_batch(batch_size)

    def save(self, path: pathlib.Path):
        with open(path, "wb") as f:
            pickle.dump(self, f)
